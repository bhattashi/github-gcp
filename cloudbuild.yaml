# cloudbuild.yaml
steps:
  # 1. Create the Dataflow Template
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'Create Template'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        python dataflow_etl.py \
          --runner=DataflowRunner \
          --project=$PROJECT_ID \
          --staging_location=gs://${_GCS_BUCKET}/staging \
          --template_location=gs://${_GCS_BUCKET}/templates/etl_v1.json \
          --region=us-central1 \
          --setup_file=./setup.py \
          --save_main_session=True

  # 2. Upload the Metadata file so the UI works
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'Deploy Metadata'
    args: ['cp', 'metadata.json', 'gs://${_GCS_BUCKET}/templates/etl_v1.json_metadata']

  # 3. Deploy the Airflow DAG to Composer
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'Deploy DAG'
    args: ['cp', 'orchestration/dataflow_dag.py', 'gs://${_COMPOSER_BUCKET}/dags/']

substitutions:
  _GCS_BUCKET: 'your-dataflow-bucket'
  _COMPOSER_BUCKET: 'your-composer-dags-bucket'